# @package _global_

# specify here default prediction configuration
defaults:
  - _self_
  - dataset: conll2003
  - serializer: json
  - hydra: default.yaml
  - paths: default.yaml

pipeline_type: "prediction"

print_config: True

ignore_warnings: True

seed: null

name: "default"

# this should be the value of save_dir from the train config
# or the url to huggingface hub where the taskmodule and model was pushed to
model_name_or_path: pie/example-ner-spanclf-conll03

# specify the PyTorch-IE pipeline
pipeline:
  _target_: pytorch_ie.auto.AutoPipeline.from_pretrained
  show_progress_bar: true

# to override model weights with content of a checkpoint
ckpt_path: null

# which split from the loaded dataset will be used
dataset_split: test

# this will be passed to the serializer as "path" parameter
out_path: predictions/${name}/${dataset_split}/${now:%Y-%m-%d_%H-%M-%S}.jsonl

# if set, the final config including resolved paths etc. will be written to this location
config_out_path: ${out_path}.config.yaml
